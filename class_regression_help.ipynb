{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Lasso\n",
    "#J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x1         x2         x3         x0  x4         x5          x6  \\\n",
      "0  56.590866 -15.164974 -71.868156 -24952.783   1  29.327799  1122.86820   \n",
      "1  10.907610 -42.359173  -8.457231 -24962.082   1  90.882484   960.58179   \n",
      "2  34.974998  42.709808 -15.151949 -25028.438   0 -70.219116   986.54913   \n",
      "\n",
      "          x7          x8            y  \n",
      "0  -5.556481  122.028500  -74025800.0  \n",
      "1 -17.387789  -65.950005 -167792768.0  \n",
      "2  38.034069  -98.859238  136613888.0  \n",
      "               x1          x2          x3            x0          x4  \\\n",
      "count  516.000000  516.000000  516.000000    516.000000  516.000000   \n",
      "mean    51.736012   10.975219   11.932317 -25004.803167    0.525194   \n",
      "std     28.423105   33.699373   39.577027     51.363376    0.499849   \n",
      "min      0.081017  -81.257637 -134.974990 -25160.113000    0.000000   \n",
      "25%     29.936390  -12.377698  -14.996227 -25039.551500    0.000000   \n",
      "50%     51.267094   11.625733   13.688730 -25006.647500    1.000000   \n",
      "75%     77.361137   31.714382   38.983266 -24969.142000    1.000000   \n",
      "max     99.809120  114.399220  118.005890 -24807.332000    1.000000   \n",
      "\n",
      "               x5           x6          x7          x8             y  \n",
      "count  516.000000   516.000000  516.000000  516.000000  5.160000e+02  \n",
      "mean    27.319595  1001.325193    9.448520   25.045274 -5.365632e+07  \n",
      "std     80.863166    47.825841   25.628581   96.433169  1.621822e+08  \n",
      "min    -87.764595   861.280940  -79.024902 -215.919830 -4.213926e+08  \n",
      "25%    -49.464144   971.457760   -7.959556  -44.031105 -1.978516e+08  \n",
      "50%     22.316823  1003.310850   10.439490   25.376592 -4.096182e+07  \n",
      "75%     99.297767  1033.326050   26.924579   88.578406  9.906547e+07  \n",
      "max    202.022490  1122.868200   97.037865  329.430180  1.917787e+08  \n"
     ]
    }
   ],
   "source": [
    "# grab data -- change path for your own file\n",
    "df1 = pd.read_csv(\"C:/Users/karuthers/Dropbox/ML_teaching/classes/train3.csv\")\n",
    "\n",
    "# print first few rows -- could also use df.head()\n",
    "print (df1.iloc[:3])\n",
    "# and get summary stats on variables\n",
    "print (df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x1          x2          x3            x0          x4  \\\n",
      "count  516.000000  516.000000  516.000000    516.000000  516.000000   \n",
      "mean    51.736012   10.975219   11.932317 -25004.803167    0.525194   \n",
      "std     28.423105   33.699373   39.577027     51.363376    0.499849   \n",
      "min      0.081017  -81.257637 -134.974990 -25160.113000    0.000000   \n",
      "25%     29.936390  -12.377698  -14.996227 -25039.551500    0.000000   \n",
      "50%     51.267094   11.625733   13.688730 -25006.647500    1.000000   \n",
      "75%     77.361137   31.714382   38.983266 -24969.142000    1.000000   \n",
      "max     99.809120  114.399220  118.005890 -24807.332000    1.000000   \n",
      "\n",
      "               x5           x6          x7          x8             y  \n",
      "count  516.000000   516.000000  516.000000  516.000000  5.160000e+02  \n",
      "mean    27.319595  1001.325193    9.448520   25.045274 -5.365632e+07  \n",
      "std     80.863166    47.825841   25.628581   96.433169  1.621822e+08  \n",
      "min    -87.764595   861.280940  -79.024902 -215.919830 -4.213926e+08  \n",
      "25%    -49.464144   971.457760   -7.959556  -44.031105 -1.978516e+08  \n",
      "50%     22.316823  1003.310850   10.439490   25.376592 -4.096182e+07  \n",
      "75%     99.297767  1033.326050   26.924579   88.578406  9.906547e+07  \n",
      "max    202.022490  1122.868200   97.037865  329.430180  1.917787e+08  \n"
     ]
    }
   ],
   "source": [
    "df2 = df1.dropna()\n",
    "# and get summary stats on variables to check to see if any variables had missingness\n",
    "print (df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of variables to pass to PCA, x's only / exclude Y\n",
    "vars = ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']\n",
    "x = df2.loc[:, vars].values\n",
    "\n",
    "# also create Y while we're at it for use later on in regressions\n",
    "y = df2.loc[:, 'y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize x\n",
    "x_norm = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "pca1 = PCA(n_components=dim)\n",
    "# create n dimensional representation\n",
    "latent_vars = pca1.fit_transform(x_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each latent variable in PCA:  [0.25896365 0.21282259 0.12417551 0.1163428  0.10375013 0.09874006\n",
      " 0.05700854 0.01877002]\n",
      "\n",
      "\n",
      "x 0 : 0.0425 , 0.0131 , -0.4741 , 0.5572 , 0.5263 , -0.4167 , 0.1092 , 0.009 , \n",
      "\n",
      "x 1 : 0.0173 , -0.0601 , 0.0796 , 0.7421 , -0.6584 , 0.0588 , -0.0416 , 0.0014 , \n",
      "\n",
      "x 2 : -0.5766 , -0.1159 , -0.0393 , 0.0383 , 0.0602 , -0.0326 , -0.4897 , -0.6376 , \n",
      "\n",
      "x 3 : -0.5091 , -0.0929 , -0.035 , -0.0657 , -0.141 , -0.07 , 0.8211 , -0.1651 , \n",
      "\n",
      "x 4 : -0.145 , 0.6847 , 0.0727 , 0.0295 , -0.0048 , -0.0627 , -0.027 , 0.0166 , \n",
      "\n",
      "x 5 : -0.1507 , 0.6833 , 0.0633 , 0.0389 , -0.0429 , -0.0542 , -0.0009 , 0.0208 , \n",
      "\n",
      "x 6 : 0.0327 , -0.1213 , 0.6446 , -0.0155 , 0.0065 , -0.7533 , -0.0276 , 0.0122 , \n",
      "\n",
      "x 7 : -0.597 , -0.1517 , -0.0311 , 0.0198 , 0.0322 , -0.0075 , -0.2311 , 0.7515 , \n",
      "\n",
      "x 8 : -0.0707 , -0.028 , 0.5834 , 0.3606 , 0.5129 , 0.4925 , 0.1323 , -0.0235 , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Variance explained by each latent variable in PCA: \", pca1.explained_variance_ratio_)\n",
    "print (\"\\n\")\n",
    "\n",
    "for i in range(0,9):\n",
    "    print (\"x\",i,\": \", end='')\n",
    "    for j in range(0,dim):\n",
    "        print (round(pca1.components_[j][i],4), \", \", end='')\n",
    "    print (\"\\n\")\n",
    "\n",
    "# clear from the component scores / variance explained by each dimension that x1,x2,x3 are loading on the first factor\n",
    "# also possible that x4 and x5 are related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91423837],\n",
       "       [0.91423837, 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check x4 and x5 -- only keep x5 b/c it has more variance\n",
    "np.corrcoef(df2['x4'], df2['x5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each latent variable in PCA:  [0.75074945]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new pca focusing on 1 factor, just using x7,x2,x3\n",
    "vars2 = ['x2', 'x3', 'x7']\n",
    "temp = df2.loc[:, vars2].values\n",
    "pca2 = PCA(n_components=1)\n",
    "\n",
    "# create 1 dimensional representation\n",
    "latent_vars = pca2.fit_transform(temp)\n",
    "\n",
    "# check to see if I'm right\n",
    "print (\"Variance explained by each latent variable in PCA: \", pca2.explained_variance_ratio_)\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42.709808 -15.151949  38.034069]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      56.590866\n",
       "1      10.907610\n",
       "2      34.974998\n",
       "3      99.809120\n",
       "4      26.966738\n",
       "         ...    \n",
       "511    87.572922\n",
       "512     0.204052\n",
       "513    39.199738\n",
       "514    48.779186\n",
       "515    77.008270\n",
       "Name: x1, Length: 516, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(temp[2])\n",
    "df2['x1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with the latent variables from pca1\n",
    "df2['pca1'] = latent_vars[:,0]\n",
    "# add the latent variables to x_norm\n",
    "x_norm = np.append(x_norm,latent_vars,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is huge; reduce\n",
    "df2['y'] = df2['y']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56664816 -0.69305938 -0.44562155]]\n",
      "[1139.57793577]\n"
     ]
    }
   ],
   "source": [
    "print(pca2.components_)\n",
    "print(pca2.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 6) (387,)\n",
      "(129, 6) (129,)\n"
     ]
    }
   ],
   "source": [
    "IVs = ['x0', 'x1', 'x5', 'x6', 'x8', 'pca1']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# make sure results make sense\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "p2_model = LinearRegression(normalize=True)\n",
    "p3_model = LinearRegression(normalize=True)\n",
    "\n",
    "# create polynomial features\n",
    "p2_features = PolynomialFeatures(degree=2)\n",
    "p2_train = p2_features.fit_transform(x_train)\n",
    "p2_test = p2_features.fit_transform(x_test)\n",
    "\n",
    "p3_features = PolynomialFeatures(degree=3)\n",
    "p3_train = p3_features.fit_transform(x_train)\n",
    "p3_test = p3_features.fit_transform(x_test)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "p2_1 = p2_model.fit(p2_train, y_train)\n",
    "p3_1 = p3_model.fit(p3_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.52317969e-01  4.44402790e+00 -2.00688087e+03 -1.56112175e+02\n",
      " -9.63162596e+00 -1.83669692e+00]\n",
      "[ 1.77066567e+17 -7.01545455e+01  6.79793587e+02  2.18066077e+03\n",
      "  3.10579275e+02 -1.57074062e+02  5.99028193e+00 -1.31425445e-03\n",
      "  2.57727370e-02  6.66971347e-03  3.49860706e-03 -7.07677695e-03\n",
      "  1.53502432e-04 -6.09931957e-02 -2.65217721e-02 -2.95764351e-02\n",
      "  4.45034888e-02  4.94422080e-02 -9.90855677e-03 -4.01596925e+00\n",
      "  4.88640822e-03  5.98087775e-03 -1.09460274e-01 -2.21003401e-02\n",
      " -5.05066210e-03  1.37457467e-03  1.31429557e-03  5.71950523e-04]\n",
      "-1.7706656687852954e+17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'x5',\n",
       " 'x0^2',\n",
       " 'x0 x1',\n",
       " 'x0 x2',\n",
       " 'x0 x3',\n",
       " 'x0 x4',\n",
       " 'x0 x5',\n",
       " 'x1^2',\n",
       " 'x1 x2',\n",
       " 'x1 x3',\n",
       " 'x1 x4',\n",
       " 'x1 x5',\n",
       " 'x2^2',\n",
       " 'x2 x3',\n",
       " 'x2 x4',\n",
       " 'x2 x5',\n",
       " 'x3^2',\n",
       " 'x3 x4',\n",
       " 'x3 x5',\n",
       " 'x4^2',\n",
       " 'x4 x5',\n",
       " 'x5^2']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin_1.coef_)\n",
    "\n",
    "print(p2_1.coef_)\n",
    "print(p2_1.intercept_)\n",
    "p2_features.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for test sets\n",
    "lin1_predict = lin_1.predict(x_test)\n",
    "p2_predict = p2_1.predict(p2_test)\n",
    "p3_predict = p3_1.predict(p3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38719.7983378  109529.50031567 -28002.02115934 -77697.12504772\n",
      " 106530.55153601  68089.23728347 -52943.11560141 -25943.35720998\n",
      " 101476.45262714 137717.85191282]\n",
      "151     52607.792\n",
      "341     89044.552\n",
      "29     -28404.036\n",
      "0      -74025.800\n",
      "504     94448.432\n",
      "215     84948.224\n",
      "58     -54579.260\n",
      "71     -35814.480\n",
      "65      87980.104\n",
      "223    170827.376\n",
      "Name: y, dtype: float64\n",
      "\n",
      "\n",
      "151    13887.993662\n",
      "341   -20484.948316\n",
      "29      -402.014841\n",
      "0       3671.325048\n",
      "504   -12082.119536\n",
      "215    16858.986717\n",
      "58     -1636.144399\n",
      "71     -9871.122790\n",
      "65    -13496.348627\n",
      "223    33109.524087\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of errors')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IVs = ['x0', 'x1', 'x5', 'x6', 'x8', 'pca1']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "\n",
    "# ok, check first ten observations of predictions, y_test, and errors to make sure nothing is wrong\n",
    "print (lin1_predict[0:10])\n",
    "print (y_test[0:10])\n",
    "\n",
    "# this creates errors of y and y'\n",
    "errors = (y_test - lin1_predict)\n",
    "print()\n",
    "print()\n",
    "print (errors[0:10])\n",
    "\n",
    "# do histogram -- choose reasonable bins parameter\n",
    "errors.hist(bins = 40)\n",
    "plt.title('Histogram of errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "28\n",
      "84\n",
      "[ 2.52317969e-01  4.44402790e+00 -2.00688087e+03 -1.56112175e+02\n",
      " -9.63162596e+00 -1.83669692e+00]\n",
      "163819.70739437843\n",
      "Index(['x0', 'x1', 'x5', 'x6', 'x8', 'pca1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# just check that things make sense\n",
    "print (len(lin_1.coef_))\n",
    "print (len(p2_1.coef_))\n",
    "print (len(p3_1.coef_))\n",
    "print (lin_1.coef_)\n",
    "print (lin_1.intercept_)\n",
    "print (x_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98829708 0.99031867 0.98946065 0.99160136 0.99391446]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# one can do this w/ cross_val_score or kfold -- but this is easier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(linear_model, df2.loc[:, IVs], df2.loc[:, 'y'], cv=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear train / test rmse:  15293.818047282768  /  15702.238713293522\n",
      "poly degree 2 train / test rmse:  456.21966630806116  /  685.9952967729705\n",
      "poly degree 3 train / test rmse:  0.011811952447501217  /  0.015406243894747087\n",
      "linear train / test r^2:  0.9912661983730524  /  0.9900007291093141\n",
      "poly degree 2 train / test r^2:  0.9999922282380354  /  0.9999809152067503\n",
      "poly degree 3 train / test r^2:  0.9999999999999948  /  0.9999999999999903\n"
     ]
    }
   ],
   "source": [
    "# find RMSE; y_true first then y_model\n",
    "print (\"linear train / test rmse: \", mean_squared_error(y_train, lin_1.predict(x_train))**(.5), \" / \", mean_squared_error(y_test, lin1_predict)**(.5))\n",
    "print (\"poly degree 2 train / test rmse: \", mean_squared_error(y_train, p2_1.predict(p2_train))**(.5), \" / \", mean_squared_error(y_test, p2_predict)**(.5))\n",
    "print (\"poly degree 3 train / test rmse: \", mean_squared_error(y_train, p3_1.predict(p3_train))**(.5), \" / \", mean_squared_error(y_test, p3_predict)**(.5))\n",
    "\n",
    "# also do R^2\n",
    "print (\"linear train / test r^2: \", r2_score(y_train, lin_1.predict(x_train)), \" / \", r2_score(y_test, lin1_predict))\n",
    "print (\"poly degree 2 train / test r^2: \", r2_score(y_train, p2_1.predict(p2_train)), \" / \", r2_score(y_test, p2_predict))\n",
    "print (\"poly degree 3 train / test r^2: \", r2_score(y_train, p3_1.predict(p3_train)), \" / \", r2_score(y_test, p3_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 ,  0.2523179688138302\n",
      "x1 ,  4.444027898299352\n",
      "x5 ,  -2006.880867457082\n",
      "x6 ,  -156.11217496725746\n",
      "x8 ,  -9.631625955461082\n",
      "pca1 ,  -1.8366969212302555\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(IVs)):\n",
    "    print (IVs[i], \", \", (lin_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  1.7706656687748346e+17\n",
      "x0 ,  -70.15454554693072\n",
      "x1 ,  679.7935872870637\n",
      "x5 ,  2180.660769705462\n",
      "x6 ,  310.57927493630143\n",
      "x8 ,  -157.07406226581833\n",
      "pca1 ,  5.990281932001363\n",
      "x0^2 ,  -0.0013142544454199801\n",
      "x0 x1 ,  0.02577273701410956\n",
      "x0 x5 ,  0.006669713471849328\n",
      "x0 x6 ,  0.003498607058093637\n",
      "x0 x8 ,  -0.007076776950029223\n",
      "x0 pca1 ,  0.00015350243237535202\n",
      "x1^2 ,  -0.06099319567351823\n",
      "x1 x5 ,  -0.02652177214929233\n",
      "x1 x6 ,  -0.02957643509844635\n",
      "x1 x8 ,  0.04450348877362906\n",
      "x1 pca1 ,  0.04944220802555347\n",
      "x5^2 ,  -0.009908556771711412\n",
      "x5 x6 ,  -4.015969245277702\n",
      "x5 x8 ,  0.004886408219759157\n",
      "x5 pca1 ,  0.005980877752887172\n",
      "x6^2 ,  -0.10946027387212845\n",
      "x6 x8 ,  -0.022100340108421986\n",
      "x6 pca1 ,  -0.005050662099193871\n",
      "x8^2 ,  0.0013745746699983454\n",
      "x8 pca1 ,  0.0013142955738428322\n",
      "pca1^2 ,  0.0005719505225339839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p2_features.get_feature_names())):\n",
    "    print (p2_features.get_feature_names(x_train.columns)[i], \", \", (p2_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  -3.0634781323908113e-07\n",
      "x0 ,  2.5101942331158345\n",
      "x1 ,  -4.113039804025568\n",
      "x5 ,  -0.683540853415727\n",
      "x6 ,  -2.919782966167071\n",
      "x8 ,  -1.6507639291354583\n",
      "pca1 ,  3.142396881464479\n",
      "x0^2 ,  9.541413141135092e-05\n",
      "x0 x1 ,  -0.0003156816734456378\n",
      "x0 x5 ,  -5.326564007715405e-05\n",
      "x0 x6 ,  -0.00020538017943464763\n",
      "x0 x8 ,  -0.0001266467501147975\n",
      "x0 pca1 ,  0.00026309150310263027\n",
      "x1^2 ,  0.0007673632539214165\n",
      "x1 x5 ,  -0.00024223431169135596\n",
      "x1 x6 ,  0.0003385993881095243\n",
      "x1 x8 ,  0.0003477031197966801\n",
      "x1 pca1 ,  0.0002691429678511474\n",
      "x5^2 ,  -1.7814513680286047e-05\n",
      "x5 x6 ,  1.5351066488350876e-05\n",
      "x5 x8 ,  9.331121258216571e-05\n",
      "x5 pca1 ,  0.00021517466384245003\n",
      "x6^2 ,  0.0003409587832725827\n",
      "x6 x8 ,  0.0001109287852096109\n",
      "x6 pca1 ,  0.00027155290774435705\n",
      "x8^2 ,  -3.0823430010598905e-05\n",
      "x8 pca1 ,  0.00024915271008913364\n",
      "pca1^2 ,  0.0004792271527690851\n",
      "x0^3 ,  1.214756732106436e-09\n",
      "x0^2 x1 ,  -6.056221057447811e-09\n",
      "x0^2 x5 ,  -1.0640961160367243e-09\n",
      "x0^2 x6 ,  -3.5604375935084464e-09\n",
      "x0^2 x8 ,  -2.4252671996538805e-09\n",
      "x0^2 pca1 ,  5.4919261313384324e-09\n",
      "x0 x1^2 ,  -8.777720226105512e-09\n",
      "x0 x1 x5 ,  -9.721066164033341e-09\n",
      "x0 x1 x6 ,  1.3013610936765152e-08\n",
      "x0 x1 x8 ,  1.3078301722968099e-08\n",
      "x0 x1 pca1 ,  1.2821642269993306e-08\n",
      "x0 x5^2 ,  -6.073735697838303e-10\n",
      "x0 x5 x6 ,  4.4654284175574583e-10\n",
      "x0 x5 x8 ,  3.913882134380674e-09\n",
      "x0 x5 pca1 ,  8.589632309436726e-09\n",
      "x0 x6^2 ,  1.3183913140731324e-08\n",
      "x0 x6 x8 ,  4.454603486130008e-09\n",
      "x0 x6 pca1 ,  1.0566984320816931e-08\n",
      "x0 x8^2 ,  -1.2105549433565546e-09\n",
      "x0 x8 pca1 ,  1.0446682432647425e-08\n",
      "x0 pca1^2 ,  1.9282975521099477e-08\n",
      "x1^3 ,  -2.442050292158666e-08\n",
      "x1^2 x5 ,  7.50943425991543e-09\n",
      "x1^2 x6 ,  1.6377459603225064e-08\n",
      "x1^2 x8 ,  -4.524902918578554e-09\n",
      "x1^2 pca1 ,  -2.8623567338561444e-08\n",
      "x1 x5^2 ,  8.734805016649507e-09\n",
      "x1 x5 x6 ,  -2.129883177842315e-09\n",
      "x1 x5 x8 ,  -6.431121603372756e-09\n",
      "x1 x5 pca1 ,  1.391717383322086e-08\n",
      "x1 x6^2 ,  -7.266956841626448e-09\n",
      "x1 x6 x8 ,  -2.057176927950317e-08\n",
      "x1 x6 pca1 ,  5.337331670648589e-08\n",
      "x1 x8^2 ,  -1.4118699876906628e-10\n",
      "x1 x8 pca1 ,  -1.9722896343866188e-11\n",
      "x1 pca1^2 ,  3.769366085053519e-09\n",
      "x5^3 ,  -1.7352513021064473e-09\n",
      "x5^2 x6 ,  2.2458545276677185e-09\n",
      "x5^2 x8 ,  3.943183979219714e-09\n",
      "x5^2 pca1 ,  -8.221986869613375e-09\n",
      "x5 x6^2 ,  -0.0020000021561004415\n",
      "x5 x6 x8 ,  4.854398491905398e-09\n",
      "x5 x6 pca1 ,  -3.293617023400723e-10\n",
      "x5 x8^2 ,  -1.1944586989299257e-09\n",
      "x5 x8 pca1 ,  -1.1761816817104559e-09\n",
      "x5 pca1^2 ,  -2.2940111500618503e-09\n",
      "x6^3 ,  -3.724474987074717e-09\n",
      "x6^2 x8 ,  8.300339009889809e-10\n",
      "x6^2 pca1 ,  -4.999050755785409e-09\n",
      "x6 x8^2 ,  5.893909091810799e-10\n",
      "x6 x8 pca1 ,  1.1897148285994785e-08\n",
      "x6 pca1^2 ,  2.9274039188689146e-09\n",
      "x8^3 ,  -7.435090421603923e-10\n",
      "x8^2 pca1 ,  9.085497125775557e-10\n",
      "x8 pca1^2 ,  8.450634168624495e-11\n",
      "pca1^3 ,  -6.049580212800293e-10\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p3_features.get_feature_names())):\n",
    "    print (p3_features.get_feature_names(x_train.columns)[i], \", \", (p3_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.6387561195853\n",
      "345.6755759456412\n",
      "345.7199922013395\n",
      "345.7544843595552\n",
      "345.697082671252\n",
      "345.8953996884159\n",
      "346.230110297887\n",
      "348.7822530562613\n",
      "364.9235222118542\n",
      "394.3724836811102\n",
      "373.1995670748906\n"
     ]
    }
   ],
   "source": [
    "# lots of variables, but many are small -- time to use regularized regression to get rid of some\n",
    "# try lasso and test different penalties\n",
    "lambdas = (.1, .5, 1, 2.5, 5, 7.5, 10, 20, 50, 100, 200)\n",
    "\n",
    "for i in lambdas:    \n",
    "    lasso_reg = Lasso(alpha = i, max_iter=10000)\n",
    "    lasso1 = lasso_reg.fit(p3_train, y_train)\n",
    "    lasso1_predict = lasso1.predict(p3_test)\n",
    "    print (mean_squared_error(y_test, lasso1_predict)**(.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.697082671252\n"
     ]
    }
   ],
   "source": [
    "# ok, go for higher value of lambda given the above\n",
    "lasso_reg = Lasso(alpha = 5, max_iter=10000)\n",
    "lasso1 = lasso_reg.fit(p3_train, y_train)\n",
    "lasso1_predict = lasso1.predict(p3_test)\n",
    "print (mean_squared_error(y_test, lasso1_predict)**(.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  0.0\n",
      "x0 ,  7.233384834714787\n",
      "x1 ,  -0.0\n",
      "x5 ,  71.33895454355374\n",
      "x6 ,  22.71127333944473\n",
      "x8 ,  -3.9680884138175907\n",
      "pca1 ,  0.0\n",
      "x0^2 ,  -0.00020036726748782604\n",
      "x0 x1 ,  -0.0018792893905550574\n",
      "x0 x5 ,  0.0012953938250613026\n",
      "x0 x6 ,  -0.0002989333771859228\n",
      "x0 x8 ,  2.546933870391235e-05\n",
      "x0 pca1 ,  0.0002478776008707092\n",
      "x1^2 ,  -0.028747620666481573\n",
      "x1 x5 ,  0.10619887692944333\n",
      "x1 x6 ,  0.029573525260659428\n",
      "x1 x8 ,  -0.10758358631294565\n",
      "x1 pca1 ,  0.19955031155713387\n",
      "x5^2 ,  -0.005390309090460515\n",
      "x5 x6 ,  -0.990389783456325\n",
      "x5 x8 ,  0.07231834607811637\n",
      "x5 pca1 ,  0.2467588400427235\n",
      "x6^2 ,  -0.02725023910626372\n",
      "x6 x8 ,  -0.0013882590938557066\n",
      "x6 pca1 ,  -0.017936677223201938\n",
      "x8^2 ,  -0.02210045625254297\n",
      "x8 pca1 ,  -0.15916257119005814\n",
      "pca1^2 ,  0.2217577405505581\n",
      "x0^3 ,  -5.660468385348885e-09\n",
      "x0^2 x1 ,  -2.1521673497769826e-07\n",
      "x0^2 x5 ,  1.5020355905479637e-06\n",
      "x0^2 x6 ,  1.2998665956807876e-07\n",
      "x0^2 x8 ,  2.4030208946963536e-08\n",
      "x0^2 pca1 ,  5.319544023765749e-09\n",
      "x0 x1^2 ,  -3.894639515841316e-05\n",
      "x0 x1 x5 ,  -5.2888750010721525e-06\n",
      "x0 x1 x6 ,  -1.2705269181394923e-06\n",
      "x0 x1 x8 ,  2.5495760568836363e-06\n",
      "x0 x1 pca1 ,  -5.122018926442633e-06\n",
      "x0 x5^2 ,  2.653779522857819e-06\n",
      "x0 x5 x6 ,  3.92030664349633e-05\n",
      "x0 x5 x8 ,  -4.197538231473002e-07\n",
      "x0 x5 pca1 ,  2.992651581665938e-06\n",
      "x0 x6^2 ,  1.1491456833976404e-06\n",
      "x0 x6 x8 ,  1.4747736502398648e-07\n",
      "x0 x6 pca1 ,  2.6789126392604e-07\n",
      "x0 x8^2 ,  -2.2205341775437322e-07\n",
      "x0 x8 pca1 ,  -1.029626427606822e-06\n",
      "x0 pca1^2 ,  2.6645001319618114e-08\n",
      "x1^3 ,  0.0007021586702261895\n",
      "x1^2 x5 ,  0.00012647880617319706\n",
      "x1^2 x6 ,  -0.001066570891480656\n",
      "x1^2 x8 ,  -0.00015544934618295754\n",
      "x1^2 pca1 ,  0.000916635745319778\n",
      "x1 x5^2 ,  -0.00016012742755666624\n",
      "x1 x5 x6 ,  -0.00024255480756429633\n",
      "x1 x5 x8 ,  3.907734764639496e-05\n",
      "x1 x5 pca1 ,  -0.00014509943483556338\n",
      "x1 x6^2 ,  3.260129630266682e-05\n",
      "x1 x6 x8 ,  0.00019500642396012724\n",
      "x1 x6 pca1 ,  -0.00040995184478280874\n",
      "x1 x8^2 ,  -4.2076561631096605e-05\n",
      "x1 x8 pca1 ,  -0.00028127384992358335\n",
      "x1 pca1^2 ,  -7.870688644294813e-05\n",
      "x5^3 ,  2.3572655825983866e-06\n",
      "x5^2 x6 ,  7.067633630121119e-05\n",
      "x5^2 x8 ,  5.382218334657087e-05\n",
      "x5^2 pca1 ,  -1.8108390312286472e-05\n",
      "x5 x6^2 ,  -0.0010087194684682726\n",
      "x5 x6 x8 ,  -8.821763791693711e-05\n",
      "x5 x6 pca1 ,  -0.00015804156229774468\n",
      "x5 x8^2 ,  -6.798182529259552e-06\n",
      "x5 x8 pca1 ,  -6.81701642399007e-05\n",
      "x5 pca1^2 ,  -8.705053650314775e-05\n",
      "x6^3 ,  -5.210494479490809e-07\n",
      "x6^2 x8 ,  -5.84938895607742e-06\n",
      "x6^2 pca1 ,  2.887259271950957e-05\n",
      "x6 x8^2 ,  2.0873918128038625e-05\n",
      "x6 x8 pca1 ,  0.0001526487627637217\n",
      "x6 pca1^2 ,  -0.00021362518111436792\n",
      "x8^3 ,  -1.69468544741416e-05\n",
      "x8^2 pca1 ,  -9.479894117603953e-06\n",
      "x8 pca1^2 ,  -1.3473049924813178e-05\n",
      "pca1^3 ,  -8.021882697897102e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p3_features.get_feature_names())):\n",
    "    print (p3_features.get_feature_names(x_train.columns)[i], \", \", (lasso1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the above, clear that x5, x6 are the largest IV's, followed by x0 and x8; pca1 has a modest effect, some interaction of x5*x6, x7^2, and that's about it.  \n",
    "# So rerun with those and call it a day.  \n",
    "# one issue: the above only has 2nd order polys / interactions, but the above results show that the degree 3 poly did better OOS\n",
    "# either I've missed a term, or there's a bug somewhere above -- that said, this isn't quite enough data to get stable results\n",
    "# if one changes the random number seed or the proportion or train / test, things change more than I'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          x1         x2         x3         x0  x4         x5          x6  \\\n",
      "0  56.590866 -15.164974 -71.868156 -24952.783   1  29.327799  1122.86820   \n",
      "1  10.907610 -42.359173  -8.457231 -24962.082   1  90.882484   960.58179   \n",
      "2  34.974998  42.709808 -15.151949 -25028.438   0 -70.219116   986.54913   \n",
      "\n",
      "          x7          x8            y  \n",
      "0  -5.556481  122.028500  -74025800.0  \n",
      "1 -17.387789  -65.950005 -167792768.0  \n",
      "2  38.034069  -98.859238  136613888.0  \n",
      "               x1          x2          x3            x0          x4  \\\n",
      "count  516.000000  516.000000  516.000000    516.000000  516.000000   \n",
      "mean    51.736012   10.975219   11.932317 -25004.803167    0.525194   \n",
      "std     28.423105   33.699373   39.577027     51.363376    0.499849   \n",
      "min      0.081017  -81.257637 -134.974990 -25160.113000    0.000000   \n",
      "25%     29.936390  -12.377698  -14.996227 -25039.551500    0.000000   \n",
      "50%     51.267094   11.625733   13.688730 -25006.647500    1.000000   \n",
      "75%     77.361137   31.714382   38.983266 -24969.142000    1.000000   \n",
      "max     99.809120  114.399220  118.005890 -24807.332000    1.000000   \n",
      "\n",
      "               x5           x6          x7          x8             y  \n",
      "count  516.000000   516.000000  516.000000  516.000000  5.160000e+02  \n",
      "mean    27.319595  1001.325193    9.448520   25.045274 -5.365632e+07  \n",
      "std     80.863166    47.825841   25.628581   96.433169  1.621822e+08  \n",
      "min    -87.764595   861.280940  -79.024902 -215.919830 -4.213926e+08  \n",
      "25%    -49.464144   971.457760   -7.959556  -44.031105 -1.978516e+08  \n",
      "50%     22.316823  1003.310850   10.439490   25.376592 -4.096182e+07  \n",
      "75%     99.297767  1033.326050   26.924579   88.578406  9.906547e+07  \n",
      "max    202.022490  1122.868200   97.037865  329.430180  1.917787e+08  \n"
     ]
    }
   ],
   "source": [
    "# grab data -- change path for your own file\n",
    "df1 = pd.read_csv(\"train3.csv\")\n",
    "\n",
    "# print first few rows -- could also use df.head()\n",
    "print (df1.iloc[:3])\n",
    "# and get summary stats on variables\n",
    "print (df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               x1          x2          x3            x0          x4  \\\n",
      "count  516.000000  516.000000  516.000000    516.000000  516.000000   \n",
      "mean    51.736012   10.975219   11.932317 -25004.803167    0.525194   \n",
      "std     28.423105   33.699373   39.577027     51.363376    0.499849   \n",
      "min      0.081017  -81.257637 -134.974990 -25160.113000    0.000000   \n",
      "25%     29.936390  -12.377698  -14.996227 -25039.551500    0.000000   \n",
      "50%     51.267094   11.625733   13.688730 -25006.647500    1.000000   \n",
      "75%     77.361137   31.714382   38.983266 -24969.142000    1.000000   \n",
      "max     99.809120  114.399220  118.005890 -24807.332000    1.000000   \n",
      "\n",
      "               x5           x6          x7          x8             y  \n",
      "count  516.000000   516.000000  516.000000  516.000000  5.160000e+02  \n",
      "mean    27.319595  1001.325193    9.448520   25.045274 -5.365632e+07  \n",
      "std     80.863166    47.825841   25.628581   96.433169  1.621822e+08  \n",
      "min    -87.764595   861.280940  -79.024902 -215.919830 -4.213926e+08  \n",
      "25%    -49.464144   971.457760   -7.959556  -44.031105 -1.978516e+08  \n",
      "50%     22.316823  1003.310850   10.439490   25.376592 -4.096182e+07  \n",
      "75%     99.297767  1033.326050   26.924579   88.578406  9.906547e+07  \n",
      "max    202.022490  1122.868200   97.037865  329.430180  1.917787e+08  \n"
     ]
    }
   ],
   "source": [
    "df2 = df1.dropna()\n",
    "# and get summary stats on variables to check to see if any variables had missingness\n",
    "print (df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of variables to pass to PCA, x's only / exclude Y\n",
    "vars = ['x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8']\n",
    "x = df2.loc[:, vars].values\n",
    "\n",
    "# also create Y while we're at it for use later on in regressions\n",
    "y = df2.loc[:, 'y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize x\n",
    "x_norm = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 8\n",
    "pca1 = PCA(n_components=dim)\n",
    "# create n dimensional representation\n",
    "latent_vars = pca1.fit_transform(x_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each latent variable in PCA:  [0.25896365 0.21282259 0.12417551 0.1163428  0.10375013 0.09874006\n",
      " 0.05700854 0.01877002]\n",
      "\n",
      "\n",
      "x 0 : 0.0425 , 0.0131 , -0.4741 , 0.5572 , 0.5263 , -0.4167 , 0.1092 , 0.009 , \n",
      "\n",
      "x 1 : 0.0173 , -0.0601 , 0.0796 , 0.7421 , -0.6584 , 0.0588 , -0.0416 , 0.0014 , \n",
      "\n",
      "x 2 : -0.5766 , -0.1159 , -0.0393 , 0.0383 , 0.0602 , -0.0326 , -0.4897 , -0.6376 , \n",
      "\n",
      "x 3 : -0.5091 , -0.0929 , -0.035 , -0.0657 , -0.141 , -0.07 , 0.8211 , -0.1651 , \n",
      "\n",
      "x 4 : -0.145 , 0.6847 , 0.0727 , 0.0295 , -0.0048 , -0.0627 , -0.027 , 0.0166 , \n",
      "\n",
      "x 5 : -0.1507 , 0.6833 , 0.0633 , 0.0389 , -0.0429 , -0.0542 , -0.0009 , 0.0208 , \n",
      "\n",
      "x 6 : 0.0327 , -0.1213 , 0.6446 , -0.0155 , 0.0065 , -0.7533 , -0.0276 , 0.0122 , \n",
      "\n",
      "x 7 : -0.597 , -0.1517 , -0.0311 , 0.0198 , 0.0322 , -0.0075 , -0.2311 , 0.7515 , \n",
      "\n",
      "x 8 : -0.0707 , -0.028 , 0.5834 , 0.3606 , 0.5129 , 0.4925 , 0.1323 , -0.0235 , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (\"Variance explained by each latent variable in PCA: \", pca1.explained_variance_ratio_)\n",
    "print (\"\\n\")\n",
    "\n",
    "for i in range(0,9):\n",
    "    print (\"x\",i,\": \", end='')\n",
    "    for j in range(0,dim):\n",
    "        print (round(pca1.components_[j][i],4), \", \", end='')\n",
    "    print (\"\\n\")\n",
    "\n",
    "# clear from the component scores / variance explained by each dimension that x1,x2,x3 are loading on the first factor\n",
    "# also possible that x4 and x5 are related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.91423837],\n",
       "       [0.91423837, 1.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check x4 and x5 -- only keep x5 b/c it has more variance\n",
    "np.corrcoef(df2['x4'], df2['x5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by each latent variable in PCA:  [0.75074945]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new pca focusing on 1 factor, just using x7,x2,x3\n",
    "vars2 = ['x2', 'x3', 'x7']\n",
    "temp = df2.loc[:, vars2].values\n",
    "pca2 = PCA(n_components=1)\n",
    "\n",
    "# create 1 dimensional representation\n",
    "latent_vars = pca2.fit_transform(temp)\n",
    "\n",
    "# check to see if I'm right\n",
    "print (\"Variance explained by each latent variable in PCA: \", pca2.explained_variance_ratio_)\n",
    "print (\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42.709808 -15.151949  38.034069]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      56.590866\n",
       "1      10.907610\n",
       "2      34.974998\n",
       "3      99.809120\n",
       "4      26.966738\n",
       "         ...    \n",
       "511    87.572922\n",
       "512     0.204052\n",
       "513    39.199738\n",
       "514    48.779186\n",
       "515    77.008270\n",
       "Name: x1, Length: 516, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(temp[2])\n",
    "df2['x1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new dataframe with the latent variables from pca1\n",
    "df2['pca1'] = latent_vars[:,0]\n",
    "# add the latent variables to x_norm\n",
    "x_norm = np.append(x_norm,latent_vars,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is huge; reduce\n",
    "df2['y'] = df2['y']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.56664816 -0.69305938 -0.44562155]]\n",
      "[1139.57793577]\n"
     ]
    }
   ],
   "source": [
    "print(pca2.components_)\n",
    "print(pca2.singular_values_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(387, 6) (387,)\n",
      "(129, 6) (129,)\n"
     ]
    }
   ],
   "source": [
    "IVs = ['x0', 'x1', 'x5', 'x6', 'x8', 'pca1']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# make sure results make sense\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "p2_model = LinearRegression(normalize=True)\n",
    "p3_model = LinearRegression(normalize=True)\n",
    "\n",
    "# create polynomial features\n",
    "p2_features = PolynomialFeatures(degree=2)\n",
    "p2_train = p2_features.fit_transform(x_train)\n",
    "p2_test = p2_features.fit_transform(x_test)\n",
    "\n",
    "p3_features = PolynomialFeatures(degree=3)\n",
    "p3_train = p3_features.fit_transform(x_train)\n",
    "p3_test = p3_features.fit_transform(x_test)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "p2_1 = p2_model.fit(p2_train, y_train)\n",
    "p3_1 = p3_model.fit(p3_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.52317969e-01  4.44402790e+00 -2.00688087e+03 -1.56112175e+02\n",
      " -9.63162596e+00 -1.83669692e+00]\n",
      "[ 1.77066567e+17 -7.01545455e+01  6.79793587e+02  2.18066077e+03\n",
      "  3.10579275e+02 -1.57074062e+02  5.99028193e+00 -1.31425445e-03\n",
      "  2.57727370e-02  6.66971347e-03  3.49860706e-03 -7.07677695e-03\n",
      "  1.53502432e-04 -6.09931957e-02 -2.65217721e-02 -2.95764351e-02\n",
      "  4.45034888e-02  4.94422080e-02 -9.90855677e-03 -4.01596925e+00\n",
      "  4.88640822e-03  5.98087775e-03 -1.09460274e-01 -2.21003401e-02\n",
      " -5.05066210e-03  1.37457467e-03  1.31429557e-03  5.71950523e-04]\n",
      "-1.7706656687837818e+17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'x5',\n",
       " 'x0^2',\n",
       " 'x0 x1',\n",
       " 'x0 x2',\n",
       " 'x0 x3',\n",
       " 'x0 x4',\n",
       " 'x0 x5',\n",
       " 'x1^2',\n",
       " 'x1 x2',\n",
       " 'x1 x3',\n",
       " 'x1 x4',\n",
       " 'x1 x5',\n",
       " 'x2^2',\n",
       " 'x2 x3',\n",
       " 'x2 x4',\n",
       " 'x2 x5',\n",
       " 'x3^2',\n",
       " 'x3 x4',\n",
       " 'x3 x5',\n",
       " 'x4^2',\n",
       " 'x4 x5',\n",
       " 'x5^2']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin_1.coef_)\n",
    "\n",
    "print(p2_1.coef_)\n",
    "print(p2_1.intercept_)\n",
    "p2_features.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for test sets\n",
    "lin1_predict = lin_1.predict(x_test)\n",
    "p2_predict = p2_1.predict(p2_test)\n",
    "p3_predict = p3_1.predict(p3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 38719.7983378  109529.50031567 -28002.02115934 -77697.12504772\n",
      " 106530.55153601  68089.23728347 -52943.11560141 -25943.35720998\n",
      " 101476.45262714 137717.85191282]\n",
      "151     52607.792\n",
      "341     89044.552\n",
      "29     -28404.036\n",
      "0      -74025.800\n",
      "504     94448.432\n",
      "215     84948.224\n",
      "58     -54579.260\n",
      "71     -35814.480\n",
      "65      87980.104\n",
      "223    170827.376\n",
      "Name: y, dtype: float64\n",
      "\n",
      "\n",
      "151    13887.993662\n",
      "341   -20484.948316\n",
      "29      -402.014841\n",
      "0       3671.325048\n",
      "504   -12082.119536\n",
      "215    16858.986717\n",
      "58     -1636.144399\n",
      "71     -9871.122790\n",
      "65    -13496.348627\n",
      "223    33109.524087\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of errors')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IVs = ['x0', 'x1', 'x5', 'x6', 'x8', 'pca1']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "\n",
    "# ok, check first ten observations of predictions, y_test, and errors to make sure nothing is wrong\n",
    "print (lin1_predict[0:10])\n",
    "print (y_test[0:10])\n",
    "\n",
    "# this creates errors of y and y'\n",
    "errors = (y_test - lin1_predict)\n",
    "print()\n",
    "print()\n",
    "print (errors[0:10])\n",
    "\n",
    "# do histogram -- choose reasonable bins parameter\n",
    "errors.hist(bins = 40)\n",
    "plt.title('Histogram of errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "28\n",
      "84\n",
      "[ 2.52317969e-01  4.44402790e+00 -2.00688087e+03 -1.56112175e+02\n",
      " -9.63162596e+00 -1.83669692e+00]\n",
      "163819.70739434665\n",
      "Index(['x0', 'x1', 'x5', 'x6', 'x8', 'pca1'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# just check that things make sense\n",
    "print (len(lin_1.coef_))\n",
    "print (len(p2_1.coef_))\n",
    "print (len(p3_1.coef_))\n",
    "print (lin_1.coef_)\n",
    "print (lin_1.intercept_)\n",
    "print (x_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98829708 0.99031867 0.98946065 0.99160136 0.99391446]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# one can do this w/ cross_val_score or kfold -- but this is easier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(linear_model, df2.loc[:, IVs], df2.loc[:, 'y'], cv=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear train / test rmse:  15293.818047282766  /  15702.23871329352\n",
      "poly degree 2 train / test rmse:  459.6414498467199  /  683.8298801325353\n",
      "poly degree 3 train / test rmse:  0.011811952448284814  /  0.01540624383809224\n",
      "linear train / test r^2:  0.9912661983730524  /  0.9900007291093141\n",
      "poly degree 2 train / test r^2:  0.9999921112197746  /  0.9999810355029193\n",
      "poly degree 3 train / test r^2:  0.9999999999999948  /  0.9999999999999903\n"
     ]
    }
   ],
   "source": [
    "# find RMSE; y_true first then y_model\n",
    "print (\"linear train / test rmse: \", mean_squared_error(y_train, lin_1.predict(x_train))**(.5), \" / \", mean_squared_error(y_test, lin1_predict)**(.5))\n",
    "print (\"poly degree 2 train / test rmse: \", mean_squared_error(y_train, p2_1.predict(p2_train))**(.5), \" / \", mean_squared_error(y_test, p2_predict)**(.5))\n",
    "print (\"poly degree 3 train / test rmse: \", mean_squared_error(y_train, p3_1.predict(p3_train))**(.5), \" / \", mean_squared_error(y_test, p3_predict)**(.5))\n",
    "\n",
    "# also do R^2\n",
    "print (\"linear train / test r^2: \", r2_score(y_train, lin_1.predict(x_train)), \" / \", r2_score(y_test, lin1_predict))\n",
    "print (\"poly degree 2 train / test r^2: \", r2_score(y_train, p2_1.predict(p2_train)), \" / \", r2_score(y_test, p2_predict))\n",
    "print (\"poly degree 3 train / test r^2: \", r2_score(y_train, p3_1.predict(p3_train)), \" / \", r2_score(y_test, p3_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0 ,  0.25231796881258506\n",
      "x1 ,  4.444027898302477\n",
      "x5 ,  -2006.8808674570826\n",
      "x6 ,  -156.11217496725698\n",
      "x8 ,  -9.631625955461525\n",
      "pca1 ,  -1.8366969212284576\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(IVs)):\n",
    "    print (IVs[i], \", \", (lin_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  1.770665668773321e+17\n",
      "x0 ,  -70.15454554685627\n",
      "x1 ,  679.793587286515\n",
      "x5 ,  2180.660769705725\n",
      "x6 ,  310.5792749369931\n",
      "x8 ,  -157.07406226583083\n",
      "pca1 ,  5.990281931997503\n",
      "x0^2 ,  -0.0013142544454179607\n",
      "x0 x1 ,  0.025772737014089057\n",
      "x0 x5 ,  0.006669713471861077\n",
      "x0 x6 ,  0.0034986070581210903\n",
      "x0 x8 ,  -0.007076776950029621\n",
      "x0 pca1 ,  0.00015350243237539037\n",
      "x1^2 ,  -0.06099319567351464\n",
      "x1 x5 ,  -0.026521772149281902\n",
      "x1 x6 ,  -0.029576435098409487\n",
      "x1 x8 ,  0.044503488773583316\n",
      "x1 pca1 ,  0.0494422080254903\n",
      "x5^2 ,  -0.00990855677170889\n",
      "x5 x6 ,  -4.015969245277674\n",
      "x5 x8 ,  0.004886408219769319\n",
      "x5 pca1 ,  0.005980877752886879\n",
      "x6^2 ,  -0.10946027387213367\n",
      "x6 x8 ,  -0.022100340108417688\n",
      "x6 pca1 ,  -0.005050662099191366\n",
      "x8^2 ,  0.00137457467000463\n",
      "x8 pca1 ,  0.0013142955738389564\n",
      "pca1^2 ,  0.0005719505225441596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p2_features.get_feature_names())):\n",
    "    print (p2_features.get_feature_names(x_train.columns)[i], \", \", (p2_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  -3.0634781795620137e-07\n",
      "x0 ,  2.51019424501892\n",
      "x1 ,  -4.113039964022143\n",
      "x5 ,  -0.6835408653592757\n",
      "x6 ,  -2.919782965561549\n",
      "x8 ,  -1.650763918378653\n",
      "pca1 ,  3.142396909889968\n",
      "x0^2 ,  9.541413188488942e-05\n",
      "x0 x1 ,  -0.00031568168609232244\n",
      "x0 x5 ,  -5.326564104930308e-05\n",
      "x0 x6 ,  -0.0002053801793156603\n",
      "x0 x8 ,  -0.00012664674926240086\n",
      "x0 pca1 ,  0.00026309150533286943\n",
      "x1^2 ,  0.0007673632575650222\n",
      "x1 x5 ,  -0.00024223430944110425\n",
      "x1 x6 ,  0.00033859939118746125\n",
      "x1 x8 ,  0.0003477031199003027\n",
      "x1 pca1 ,  0.0002691429690289481\n",
      "x5^2 ,  -1.7814513046333376e-05\n",
      "x5 x6 ,  1.5351066127689813e-05\n",
      "x5 x8 ,  9.331121245497903e-05\n",
      "x5 pca1 ,  0.0002151746629031189\n",
      "x6^2 ,  0.0003409587839990664\n",
      "x6 x8 ,  0.00011092878480172589\n",
      "x6 pca1 ,  0.00027155290651742514\n",
      "x8^2 ,  -3.0823429478603966e-05\n",
      "x8 pca1 ,  0.0002491527108573692\n",
      "pca1^2 ,  0.0004792271537113997\n",
      "x0^3 ,  1.214756738405503e-09\n",
      "x0^2 x1 ,  -6.056221307174838e-09\n",
      "x0^2 x5 ,  -1.06409613579718e-09\n",
      "x0^2 x6 ,  -3.5604375898469004e-09\n",
      "x0^2 x8 ,  -2.425267182761171e-09\n",
      "x0^2 pca1 ,  5.491926175061967e-09\n",
      "x0 x1^2 ,  -8.77772007592628e-09\n",
      "x0 x1 x5 ,  -9.721066072009409e-09\n",
      "x0 x1 x6 ,  1.3013611066039175e-08\n",
      "x0 x1 x8 ,  1.3078301726241187e-08\n",
      "x0 x1 pca1 ,  1.2821642316255744e-08\n",
      "x0 x5^2 ,  -6.073735454124438e-10\n",
      "x0 x5 x6 ,  4.465428279623262e-10\n",
      "x0 x5 x8 ,  3.9138821291935125e-09\n",
      "x0 x5 pca1 ,  8.589632270141016e-09\n",
      "x0 x6^2 ,  1.3183913166329333e-08\n",
      "x0 x6 x8 ,  4.454603470140071e-09\n",
      "x0 x6 pca1 ,  1.056698427154533e-08\n",
      "x0 x8^2 ,  -1.2105549220850288e-09\n",
      "x0 x8 pca1 ,  1.0446682464518288e-08\n",
      "x0 pca1^2 ,  1.928297556207457e-08\n",
      "x1^3 ,  -2.4420502197788176e-08\n",
      "x1^2 x5 ,  7.509434377963957e-09\n",
      "x1^2 x6 ,  1.637745959864537e-08\n",
      "x1^2 x8 ,  -4.52490295043043e-09\n",
      "x1^2 pca1 ,  -2.862356759044983e-08\n",
      "x1 x5^2 ,  8.734805030289214e-09\n",
      "x1 x5 x6 ,  -2.1298831330361546e-09\n",
      "x1 x5 x8 ,  -6.431121511159124e-09\n",
      "x1 x5 pca1 ,  1.3917173716110971e-08\n",
      "x1 x6^2 ,  -7.266956772735024e-09\n",
      "x1 x6 x8 ,  -2.0571769308423784e-08\n",
      "x1 x6 pca1 ,  5.3373316707245875e-08\n",
      "x1 x8^2 ,  -1.4118693496776242e-10\n",
      "x1 x8 pca1 ,  -1.9722730230965283e-11\n",
      "x1 pca1^2 ,  3.7693665389041446e-09\n",
      "x5^3 ,  -1.7352512591569522e-09\n",
      "x5^2 x6 ,  2.2458545024576275e-09\n",
      "x5^2 x8 ,  3.943183976882393e-09\n",
      "x5^2 pca1 ,  -8.221986918996725e-09\n",
      "x5 x6^2 ,  -0.0020000021561004354\n",
      "x5 x6 x8 ,  4.8543984850368235e-09\n",
      "x5 x6 pca1 ,  -3.2936174399773576e-10\n",
      "x5 x8^2 ,  -1.194458639096246e-09\n",
      "x5 x8 pca1 ,  -1.1761815505910167e-09\n",
      "x5 pca1^2 ,  -2.2940110115280708e-09\n",
      "x6^3 ,  -3.7244750174193806e-09\n",
      "x6^2 x8 ,  8.300339045855396e-10\n",
      "x6^2 pca1 ,  -4.9990507587980546e-09\n",
      "x6 x8^2 ,  5.893909135971093e-10\n",
      "x6 x8 pca1 ,  1.1897148302322326e-08\n",
      "x6 pca1^2 ,  2.9274039842978237e-09\n",
      "x8^3 ,  -7.435090440219226e-10\n",
      "x8^2 pca1 ,  9.085498074280629e-10\n",
      "x8 pca1^2 ,  8.450634629143088e-11\n",
      "pca1^3 ,  -6.049579242020663e-10\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p3_features.get_feature_names())):\n",
    "    print (p3_features.get_feature_names(x_train.columns)[i], \", \", (p3_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.6387561195419\n",
      "345.67557594564704\n",
      "345.7199922013728\n",
      "345.75448435957185\n",
      "345.6970826712076\n",
      "345.89539968834\n",
      "346.23011029795526\n",
      "348.782253056236\n",
      "364.92352221187775\n",
      "394.3724836810924\n",
      "373.19956707492514\n"
     ]
    }
   ],
   "source": [
    "# lots of variables, but many are small -- time to use regularized regression to get rid of some\n",
    "# try lasso and test different penalties\n",
    "lambdas = (.1, .5, 1, 2.5, 5, 7.5, 10, 20, 50, 100, 200)\n",
    "\n",
    "for i in lambdas:    \n",
    "    lasso_reg = Lasso(alpha = i, max_iter=10000)\n",
    "    lasso1 = lasso_reg.fit(p3_train, y_train)\n",
    "    lasso1_predict = lasso1.predict(p3_test)\n",
    "    print (mean_squared_error(y_test, lasso1_predict)**(.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345.6970826712076\n"
     ]
    }
   ],
   "source": [
    "# ok, go for higher value of lambda given the above\n",
    "lasso_reg = Lasso(alpha = 5, max_iter=10000)\n",
    "lasso1 = lasso_reg.fit(p3_train, y_train)\n",
    "lasso1_predict = lasso1.predict(p3_test)\n",
    "print (mean_squared_error(y_test, lasso1_predict)**(.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  0.0\n",
      "x0 ,  7.233384834691988\n",
      "x1 ,  -0.0\n",
      "x5 ,  71.33895454355579\n",
      "x6 ,  22.711273339459417\n",
      "x8 ,  -3.968088413824313\n",
      "pca1 ,  0.0\n",
      "x0^2 ,  -0.0002003672674880605\n",
      "x0 x1 ,  -0.0018792893905550628\n",
      "x0 x5 ,  0.0012953938250616485\n",
      "x0 x6 ,  -0.00029893337718597395\n",
      "x0 x8 ,  2.546933870378808e-05\n",
      "x0 pca1 ,  0.0002478776008691711\n",
      "x1^2 ,  -0.028747620666641736\n",
      "x1 x5 ,  0.10619887692966572\n",
      "x1 x6 ,  0.029573525260661142\n",
      "x1 x8 ,  -0.10758358631311082\n",
      "x1 pca1 ,  0.1995503115574456\n",
      "x5^2 ,  -0.005390309090409438\n",
      "x5 x6 ,  -0.9903897834563377\n",
      "x5 x8 ,  0.07231834607847037\n",
      "x5 pca1 ,  0.2467588400429937\n",
      "x6^2 ,  -0.027250239106266946\n",
      "x6 x8 ,  -0.0013882590938550693\n",
      "x6 pca1 ,  -0.017936677223243176\n",
      "x8^2 ,  -0.02210045625261302\n",
      "x8 pca1 ,  -0.15916257119025173\n",
      "pca1^2 ,  0.22175774055090983\n",
      "x0^3 ,  -5.66046838534339e-09\n",
      "x0^2 x1 ,  -2.1521673497765168e-07\n",
      "x0^2 x5 ,  1.5020355905479318e-06\n",
      "x0^2 x6 ,  1.299866595680704e-07\n",
      "x0^2 x8 ,  2.4030208946976903e-08\n",
      "x0^2 pca1 ,  5.319544023779717e-09\n",
      "x0 x1^2 ,  -3.894639515840395e-05\n",
      "x0 x1 x5 ,  -5.288875001075634e-06\n",
      "x0 x1 x6 ,  -1.2705269181396526e-06\n",
      "x0 x1 x8 ,  2.549576056887195e-06\n",
      "x0 x1 pca1 ,  -5.122018926458644e-06\n",
      "x0 x5^2 ,  2.653779522855986e-06\n",
      "x0 x5 x6 ,  3.920306643496254e-05\n",
      "x0 x5 x8 ,  -4.1975382314168e-07\n",
      "x0 x5 pca1 ,  2.99265158165791e-06\n",
      "x0 x6^2 ,  1.1491456833976008e-06\n",
      "x0 x6 x8 ,  1.474773650236837e-07\n",
      "x0 x6 pca1 ,  2.678912639280215e-07\n",
      "x0 x8^2 ,  -2.2205341775383276e-07\n",
      "x0 x8 pca1 ,  -1.0296264276115814e-06\n",
      "x0 pca1^2 ,  2.6645001319735815e-08\n",
      "x1^3 ,  0.0007021586702274548\n",
      "x1^2 x5 ,  0.00012647880617244905\n",
      "x1^2 x6 ,  -0.0010665708914805055\n",
      "x1^2 x8 ,  -0.00015544934618306374\n",
      "x1^2 pca1 ,  0.000916635745320727\n",
      "x1 x5^2 ,  -0.00016012742755754152\n",
      "x1 x5 x6 ,  -0.0002425548075644338\n",
      "x1 x5 x8 ,  3.907734764643995e-05\n",
      "x1 x5 pca1 ,  -0.00014509943483556565\n",
      "x1 x6^2 ,  3.260129630264499e-05\n",
      "x1 x6 x8 ,  0.00019500642396042372\n",
      "x1 x6 pca1 ,  -0.00040995184478358145\n",
      "x1 x8^2 ,  -4.2076561630956844e-05\n",
      "x1 x8 pca1 ,  -0.00028127384992392173\n",
      "x1 pca1^2 ,  -7.870688644299282e-05\n",
      "x5^3 ,  2.3572655825515376e-06\n",
      "x5^2 x6 ,  7.06763363011821e-05\n",
      "x5^2 x8 ,  5.3822183346430195e-05\n",
      "x5^2 pca1 ,  -1.8108390312157256e-05\n",
      "x5 x6^2 ,  -0.00100871946846826\n",
      "x5 x6 x8 ,  -8.821763791715627e-05\n",
      "x5 x6 pca1 ,  -0.00015804156229823829\n",
      "x5 x8^2 ,  -6.798182529185358e-06\n",
      "x5 x8 pca1 ,  -6.817016423976565e-05\n",
      "x5 pca1^2 ,  -8.70505365025869e-05\n",
      "x6^3 ,  -5.210494479502273e-07\n",
      "x6^2 x8 ,  -5.849388956093089e-06\n",
      "x6^2 pca1 ,  2.8872592719554526e-05\n",
      "x6 x8^2 ,  2.0873918128132486e-05\n",
      "x6 x8 pca1 ,  0.00015264876276383332\n",
      "x6 pca1^2 ,  -0.00021362518111473007\n",
      "x8^3 ,  -1.6946854474176372e-05\n",
      "x8^2 pca1 ,  -9.479894117847685e-06\n",
      "x8 pca1^2 ,  -1.3473049924706247e-05\n",
      "pca1^3 ,  -8.021882697890225e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p3_features.get_feature_names())):\n",
    "    print (p3_features.get_feature_names(x_train.columns)[i], \", \", (lasso1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the above, clear that x5, x6 are the largest IV's, followed by x0 and x8; pca1 has a modest effect, some interaction of x5*x6, x7^2, and that's about it.  \n",
    "# So rerun with those and call it a day.  \n",
    "# one issue: the above only has 2nd order polys / interactions, but the above results show that the degree 3 poly did better OOS\n",
    "# either I've missed a term, or there's a bug somewhere above -- that said, this isn't quite enough data to get stable results\n",
    "# if one changes the random number seed or the proportion or train / test, things change more than I'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

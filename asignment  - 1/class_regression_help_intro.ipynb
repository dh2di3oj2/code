{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "#from sklearn.linear_model import Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x1         x2         x3  x4   x5  train           y\n",
      "0  -99.551155 -133.21863 -1198.5691   0  0.0      1  3380.71610\n",
      "1 -180.243940 -147.40544 -1450.6776   0  0.0      1  4018.73290\n",
      "2  122.136280 -107.45643   283.7869   0  0.0      1  -613.12384\n",
      "                x1          x2           x3          x4          x5  \\\n",
      "count   277.000000  277.000000   277.000000  277.000000  277.000000   \n",
      "mean    -27.100527  -19.139597   104.186919    0.357401  -20.794985   \n",
      "std     609.832620  102.445197  1028.155514    0.480102   39.701123   \n",
      "min   -9999.000000 -286.343050 -2838.069300    0.000000 -188.637150   \n",
      "25%     -61.044746  -86.558517  -551.823850    0.000000  -36.172554   \n",
      "50%       7.100893  -12.620237   102.547520    0.000000    0.000000   \n",
      "75%      82.980598   47.833904   804.191410    1.000000    0.000000   \n",
      "max     263.999080  272.811490  2940.537800    1.000000   52.763409   \n",
      "\n",
      "            train            y  \n",
      "count  277.000000   277.000000  \n",
      "mean     0.996390  -252.757851  \n",
      "std      0.060084  2798.905854  \n",
      "min      0.000000 -8847.743200  \n",
      "25%      1.000000 -2044.081100  \n",
      "50%      1.000000  -256.244750  \n",
      "75%      1.000000  1511.171500  \n",
      "max      1.000000  8463.665000  \n"
     ]
    }
   ],
   "source": [
    "# grab data -- change path for your own file\n",
    "df1 = pd.read_csv(\"C:/Users/karuthers/Dropbox/ML_teaching/classes/train_ml1.csv\") \n",
    "\n",
    "# print first few rows -- could also use df.head()\n",
    "print (df1.iloc[:3])\n",
    "# and get summary stats on variables\n",
    "print (df1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                x1          x2           x3          x4          x5  \\\n",
      "count   277.000000  277.000000   277.000000  277.000000  277.000000   \n",
      "mean    -27.100527  -19.139597   104.186919    0.357401  -20.794985   \n",
      "std     609.832620  102.445197  1028.155514    0.480102   39.701123   \n",
      "min   -9999.000000 -286.343050 -2838.069300    0.000000 -188.637150   \n",
      "25%     -61.044746  -86.558517  -551.823850    0.000000  -36.172554   \n",
      "50%       7.100893  -12.620237   102.547520    0.000000    0.000000   \n",
      "75%      82.980598   47.833904   804.191410    1.000000    0.000000   \n",
      "max     263.999080  272.811490  2940.537800    1.000000   52.763409   \n",
      "\n",
      "            train            y  \n",
      "count  277.000000   277.000000  \n",
      "mean     0.996390  -252.757851  \n",
      "std      0.060084  2798.905854  \n",
      "min      0.000000 -8847.743200  \n",
      "25%      1.000000 -2044.081100  \n",
      "50%      1.000000  -256.244750  \n",
      "75%      1.000000  1511.171500  \n",
      "max      1.000000  8463.665000  \n"
     ]
    }
   ],
   "source": [
    "# drop missing values -- but beware that you don't drop too many rows or drop rows that are of a 'type'!  So check the count of each.\n",
    "df2 = df1.dropna()\n",
    "# and get summary stats on variables to check to see if any variables had missingness\n",
    "print (df2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of variables to pass to PCA, x's only / exclude Y\n",
    "vars = ['x1', 'x2', 'x3', 'x4', 'x5', 'train']\n",
    "x = df2.loc[:, vars].values\n",
    "\n",
    "# also create Y while we're at it for use later on in regressions\n",
    "y = df2.loc[:, 'y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -0.11901915,  -1.11557705,  -1.26937407,  -0.74577461,\n",
       "          0.52473638],\n",
       "       [ -0.2515782 ,  -1.25430964,  -1.51502252,  -0.74577461,\n",
       "          0.52473638],\n",
       "       [  0.24516058,  -0.8636489 ,   0.17499789,  -0.74577461,\n",
       "          0.52473638],\n",
       "       ...,\n",
       "       [ -0.07541315,   0.22108256,   0.39206374,   1.34088769,\n",
       "          0.52473638],\n",
       "       [-16.38145919,  -1.15449164,   1.43760471,  -0.74577461,\n",
       "          0.52473638],\n",
       "       [  0.02166346,  -1.45038384,   0.22660137,  -0.74577461,\n",
       "          0.52473638]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize x -- very useful for some algorithms. \n",
    "# x_norm = StandardScaler().fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(207, 6) (207,)\n",
      "(70, 6) (70,)\n"
     ]
    }
   ],
   "source": [
    "IVs = ['x1', 'x2', 'x3', 'x4', 'x5', 'train']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# make sure results make sense\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "p2_model = LinearRegression(normalize=True)\n",
    "p3_model = LinearRegression(normalize=True)\n",
    "\n",
    "# create polynomial features\n",
    "p2_features = PolynomialFeatures(degree=2)\n",
    "p2_train = p2_features.fit_transform(x_train)\n",
    "p2_test = p2_features.fit_transform(x_test)\n",
    "\n",
    "p3_features = PolynomialFeatures(degree=3)\n",
    "p3_train = p3_features.fit_transform(x_train)\n",
    "p3_test = p3_features.fit_transform(x_test)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "p2_1 = p2_model.fit(p2_train, y_train)\n",
    "p3_1 = p3_model.fit(p3_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.26543993e-01  2.14106972e+00 -1.60168075e+00  5.35842002e+02\n",
      "  9.24458847e+00  1.31214703e+03]\n",
      "[-1.40006619e-11  9.99015941e-01 -1.76552722e-02 -1.49975711e+00\n",
      " -3.89215988e+00  2.53538762e+00  6.41935701e+02  1.94349440e-04\n",
      " -3.57084744e-06 -2.64120310e-05 -3.13905461e-02 -5.59735817e-04\n",
      "  9.99298554e-01  1.51341545e-04 -2.36438556e-06 -2.21683550e-02\n",
      " -3.10872222e-05  1.84781641e-02  3.11986737e-07  5.00530936e+00\n",
      "  2.70474643e-05 -1.50091752e+00 -3.89215988e+00 -1.76862572e-01\n",
      " -3.89215988e+00 -1.64863490e-04  2.50827804e+00  6.41935701e+02]\n",
      "-1287.821207857093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " 'x0',\n",
       " 'x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'x5',\n",
       " 'x0^2',\n",
       " 'x0 x1',\n",
       " 'x0 x2',\n",
       " 'x0 x3',\n",
       " 'x0 x4',\n",
       " 'x0 x5',\n",
       " 'x1^2',\n",
       " 'x1 x2',\n",
       " 'x1 x3',\n",
       " 'x1 x4',\n",
       " 'x1 x5',\n",
       " 'x2^2',\n",
       " 'x2 x3',\n",
       " 'x2 x4',\n",
       " 'x2 x5',\n",
       " 'x3^2',\n",
       " 'x3 x4',\n",
       " 'x3 x5',\n",
       " 'x4^2',\n",
       " 'x4 x5',\n",
       " 'x5^2']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin_1.coef_)\n",
    "\n",
    "print(p2_1.coef_)\n",
    "print(p2_1.intercept_)\n",
    "p2_features.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict values for test sets\n",
    "lin1_predict = lin_1.predict(x_test)\n",
    "p2_predict = p2_1.predict(p2_test)\n",
    "p3_predict = p3_1.predict(p3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.85093486e+03 -1.60107780e+03 -2.45964452e+03  2.14987397e+02\n",
      "  2.56511487e+03 -1.06655931e+03 -1.67533928e+03 -6.51113239e-01\n",
      "  1.43101408e+03  6.16262829e+01]\n",
      "191    4603.03420\n",
      "124   -2644.20610\n",
      "233    3163.93360\n",
      "182     957.50745\n",
      "268   -3354.26000\n",
      "63    -1384.90820\n",
      "118   -2024.56260\n",
      "34      843.91333\n",
      "255    3027.75150\n",
      "159     175.46545\n",
      "Name: y, dtype: float64\n",
      "\n",
      "\n",
      "191    2752.099340\n",
      "124   -1043.128297\n",
      "233    5623.578123\n",
      "182     742.520053\n",
      "268   -5919.374866\n",
      "63     -318.348887\n",
      "118    -349.223319\n",
      "34      844.564443\n",
      "255    1596.737422\n",
      "159     113.839167\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Histogram of errors')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IVs = ['x1', 'x2', 'x3', 'x4', 'x5', 'train']\n",
    "\n",
    "# create train / test split using dataframe\n",
    "x_train, x_test, y_train, y_test = train_test_split(df2.loc[:, IVs], df2.loc[:, 'y'], test_size=0.25, random_state=13)\n",
    "\n",
    "# try both linear and polynomial of different degrees\n",
    "linear_model = LinearRegression(normalize=True)\n",
    "\n",
    "# now do estimation of models\n",
    "lin_1 = linear_model.fit(x_train, y_train)\n",
    "\n",
    "# ok, check first ten observations of predictions, y_test, and errors to make sure nothing is wrong\n",
    "print (lin1_predict[0:10])\n",
    "print (y_test[0:10])\n",
    "\n",
    "# this creates errors of y and y'\n",
    "errors = (y_test - lin1_predict)\n",
    "print()\n",
    "print()\n",
    "print (errors[0:10])\n",
    "\n",
    "# do histogram -- choose reasonable bins parameter\n",
    "errors.hist(bins = 40)\n",
    "plt.title('Histogram of errors')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "28\n",
      "84\n",
      "[ 2.26543993e-01  2.14106972e+00 -1.60168075e+00  5.35842002e+02\n",
      "  9.24458847e+00  1.31214703e+03]\n",
      "-1357.6015496445957\n",
      "Index(['x1', 'x2', 'x3', 'x4', 'x5', 'train'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# just check that things make sense\n",
    "print (len(lin_1.coef_))\n",
    "print (len(p2_1.coef_))\n",
    "print (len(p3_1.coef_))\n",
    "print (lin_1.coef_)\n",
    "print (lin_1.intercept_)\n",
    "print (x_train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.06987295  0.28748138  0.39542206  0.12630302 -1.68951675]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\lib\\site-packages\\sklearn\\linear_model\\_base.py:141: FutureWarning: 'normalize' was deprecated in version 1.0 and will be removed in 1.2.\n",
      "If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:\n",
      "\n",
      "from sklearn.pipeline import make_pipeline\n",
      "\n",
      "model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
      "\n",
      "If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:\n",
      "\n",
      "kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}\n",
      "model.fit(X, y, **kwargs)\n",
      "\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# one can do this w/ cross_val_score or kfold -- but this is easier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(linear_model, df2.loc[:, IVs], df2.loc[:, 'y'], cv=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear train / test rmse:  2298.746229122847  /  2842.784144434322\n",
      "poly degree 2 train / test rmse:  21.2353811444265  /  22.799218541319483\n",
      "poly degree 3 train / test rmse:  30.320391106428126  /  27.775995388393422\n",
      "linear train / test r^2:  0.3623870678691343  /  -0.2695167351041512\n",
      "poly degree 2 train / test r^2:  0.9999455879387902  /  0.999918343533242\n",
      "poly degree 3 train / test r^2:  0.9998890711130719  /  0.9998788035430168\n"
     ]
    }
   ],
   "source": [
    "# find RMSE; y_true first then y_model\n",
    "print (\"linear train / test rmse: \", mean_squared_error(y_train, lin_1.predict(x_train))**(.5), \" / \", mean_squared_error(y_test, lin1_predict)**(.5))\n",
    "print (\"poly degree 2 train / test rmse: \", mean_squared_error(y_train, p2_1.predict(p2_train))**(.5), \" / \", mean_squared_error(y_test, p2_predict)**(.5))\n",
    "print (\"poly degree 3 train / test rmse: \", mean_squared_error(y_train, p3_1.predict(p3_train))**(.5), \" / \", mean_squared_error(y_test, p3_predict)**(.5))\n",
    "\n",
    "# also do R^2\n",
    "print (\"linear train / test r^2: \", r2_score(y_train, lin_1.predict(x_train)), \" / \", r2_score(y_test, lin1_predict))\n",
    "print (\"poly degree 2 train / test r^2: \", r2_score(y_train, p2_1.predict(p2_train)), \" / \", r2_score(y_test, p2_predict))\n",
    "print (\"poly degree 3 train / test r^2: \", r2_score(y_train, p3_1.predict(p3_train)), \" / \", r2_score(y_test, p3_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 ,  0.22654399306233278\n",
      "x2 ,  2.1410697239152188\n",
      "x3 ,  -1.6016807496811634\n",
      "x4 ,  535.8420018238361\n",
      "x5 ,  9.244588468595683\n",
      "train ,  1312.1470328481394\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(IVs)):\n",
    "    print (IVs[i], \", \", (lin_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  -1.400066191510925e-11\n",
      "x1 ,  0.9990159408903392\n",
      "x2 ,  -0.0176552722370562\n",
      "x3 ,  -1.4997571089519146\n",
      "x4 ,  -3.8921598775688224\n",
      "x5 ,  2.5353876196125174\n",
      "train ,  641.9357014595862\n",
      "x1^2 ,  0.00019434944005156639\n",
      "x1 x2 ,  -3.5708474397449985e-06\n",
      "x1 x3 ,  -2.6412030957320793e-05\n",
      "x1 x4 ,  -0.03139054607015939\n",
      "x1 x5 ,  -0.0005597358173919782\n",
      "x1 train ,  0.999298554367172\n",
      "x2^2 ,  0.00015134154531235697\n",
      "x2 x3 ,  -2.3643855645267625e-06\n",
      "x2 x4 ,  -0.02216835495524341\n",
      "x2 x5 ,  -3.1087222235051395e-05\n",
      "x2 train ,  0.018478164064383233\n",
      "x3^2 ,  3.119867368456216e-07\n",
      "x3 x4 ,  5.005309355447011\n",
      "x3 x5 ,  2.7047464322075977e-05\n",
      "x3 train ,  -1.5009175176387812\n",
      "x4^2 ,  -3.892159877572831\n",
      "x4 x5 ,  -0.1768625715391635\n",
      "x4 train ,  -3.892159877572831\n",
      "x5^2 ,  -0.00016486348992519716\n",
      "x5 train ,  2.5082780372759106\n",
      "train^2 ,  641.9357014596242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p2_features.get_feature_names())):\n",
    "    print (p2_features.get_feature_names(x_train.columns)[i], \", \", (p2_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ,  -1.3249003091373216e+16\n",
      "x1 ,  292275900886.7837\n",
      "x2 ,  435959010305.54913\n",
      "x3 ,  -66883284731.55029\n",
      "x4 ,  137877821709009.92\n",
      "x5 ,  2287526120917.777\n",
      "train ,  -1269676207300899.5\n",
      "x1^2 ,  -10047876.041157441\n",
      "x1 x2 ,  -1281289154.2365384\n",
      "x1 x3 ,  -249449182.31739306\n",
      "x1 x4 ,  -4267170472544.518\n",
      "x1 x5 ,  -23428198353.391876\n",
      "x1 train ,  35652621529.97898\n",
      "x2^2 ,  -2725965347.446393\n",
      "x2 x3 ,  290360139.25872046\n",
      "x2 x4 ,  -4453134380570.4\n",
      "x2 x5 ,  -4326425820.040369\n",
      "x2 train ,  798910954178.8064\n",
      "x3^2 ,  37442796.28765829\n",
      "x3 x4 ,  30721399124.44026\n",
      "x3 x5 ,  -145247846.0763601\n",
      "x3 train ,  49593712801.036995\n",
      "x4^2 ,  -34108663297040.57\n",
      "x4 x5 ,  3943386819454.6265\n",
      "x4 train ,  -121320264516040.81\n",
      "x5^2 ,  6376966453.080841\n",
      "x5 train ,  -1515948823227.4548\n",
      "train^2 ,  882000029913345.1\n",
      "x1^3 ,  -4.5655628293913977e-08\n",
      "x1^2 x2 ,  -1.1580908048668358e-06\n",
      "x1^2 x3 ,  -7.62886909425891e-08\n",
      "x1^2 x4 ,  -0.00048619901318013833\n",
      "x1^2 x5 ,  -2.0169893278243353e-05\n",
      "x1^2 train ,  10047876.040875794\n",
      "x1 x2^2 ,  -3.076186185684817e-06\n",
      "x1 x2 x3 ,  -6.02910342198848e-07\n",
      "x1 x2 x4 ,  -0.00020685596277943925\n",
      "x1 x2 x5 ,  -2.6164967625999487e-05\n",
      "x1 x2 train ,  1281289154.2358437\n",
      "x1 x3^2 ,  2.0013743251972414e-08\n",
      "x1 x3 x4 ,  0.00017206218727757884\n",
      "x1 x3 x5 ,  2.72520946589002e-06\n",
      "x1 x3 train ,  249449182.3173907\n",
      "x1 x4^2 ,  2061895989323.2078\n",
      "x1 x4 x5 ,  0.006095435101466074\n",
      "x1 x4 train ,  2205274483221.283\n",
      "x1 x5^2 ,  1.490462670105287e-07\n",
      "x1 x5 train ,  23428198353.3922\n",
      "x1 train^2 ,  -327928522414.83167\n",
      "x2^3 ,  -6.453015040820138e-07\n",
      "x2^2 x3 ,  -5.672552996565324e-07\n",
      "x2^2 x4 ,  -0.0013572858209361179\n",
      "x2^2 x5 ,  -6.238838692372477e-07\n",
      "x2^2 train ,  2725965347.4465075\n",
      "x2 x3^2 ,  1.145577346391408e-08\n",
      "x2 x3 x4 ,  8.76376948593459e-05\n",
      "x2 x3 x5 ,  -1.4809868689089488e-06\n",
      "x2 x3 train ,  -290360139.25866497\n",
      "x2 x4^2 ,  2226567190285.021\n",
      "x2 x4 x5 ,  -0.001697171975500377\n",
      "x2 x4 train ,  2226567190285.021\n",
      "x2 x5^2 ,  3.288160170223249e-05\n",
      "x2 x5 train ,  4326425820.045629\n",
      "x2 train^2 ,  -1234869964484.3198\n",
      "x3^3 ,  -9.518621746638922e-10\n",
      "x3^2 x4 ,  2.6489955829102634e-06\n",
      "x3^2 x5 ,  -4.909214624013083e-08\n",
      "x3^2 train ,  -37442796.28765858\n",
      "x3 x4^2 ,  -94868913171.13737\n",
      "x3 x4 x5 ,  -0.00023398322532478083\n",
      "x3 x4 train ,  64147514051.69442\n",
      "x3 x5^2 ,  1.5114048502254477e-08\n",
      "x3 x5 train ,  145247846.0764886\n",
      "x3 train^2 ,  17289571927.527935\n",
      "x4^3 ,  -22913239334989.574\n",
      "x4^2 x5 ,  -2889139674887.524\n",
      "x4^2 train ,  -22913239334989.574\n",
      "x4 x5^2 ,  -0.001582787434578442\n",
      "x4 x5 train ,  -1054247144568.1396\n",
      "x4 train^2 ,  63377584774028.266\n",
      "x5^3 ,  2.852231232735483e-06\n",
      "x5^2 train ,  -6376966453.082037\n",
      "x5 train^2 ,  -771577297685.5724\n",
      "train^3 ,  389909682886487.1\n"
     ]
    }
   ],
   "source": [
    "# now, look for large magnitude IVs -- note trick with get_features_names() to have columns from original data\n",
    "for i in range(0, len(p3_features.get_feature_names())):\n",
    "    print (p3_features.get_feature_names(x_train.columns)[i], \", \", (p3_1.coef_)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add writeup here on what variables matter / what your final model is for Y ~ f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
